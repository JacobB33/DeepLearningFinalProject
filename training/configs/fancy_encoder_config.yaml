trainer_config:
  max_epochs: 120
  batch_size: 512
  data_loader_workers: 12
  grad_norm_clip: 1.0
  snapshot_path: fancy-model-2.pt
  save_every: 15
  use_amp: False
  use_wandb: True
  run_name: fancy-model-2
  use_lr_scheduler: True
lr_scheduler_config:
  type: plateau
  gamma: 0.5
  patience: 10
  mode: min
  threshold: 0.004
  min_lr: 0.00001
  cooldown: 4
optimizer_config:
  optimizer: adam
  weight_decay: 0.0
  learning_rate: 0.001
data_config:
  train_percentage: 0.98
model_config:
  upscale_schedule: [8, 16, 32, 40, 48, 64, 72, 77]
  nhead: -1
  num_transformer_layers: -1
compile: False
model_type: fancy
